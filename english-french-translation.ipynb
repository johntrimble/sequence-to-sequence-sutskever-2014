{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import (Embedding, TimeDistributed, Dense,\n",
    "                          Input, LSTM, Activation, Lambda)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import util\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Sutskever et al.'s results from \"Sequence to Sequence Learning\"\n",
    "John Trimble <trimblej@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we attempt to reproduce Sutskever et al.'s (2014) neural machine translation model and results on the ntst14 dataset for a single model (i.e. not the results for the ensemble networks). We focus on the modifications of the model to make it trainable on a single 8GB GPU and the implementation details of building the model, training it, and producing translations. Since we will be referring to Sutskever et al. (2014) quite frequently throughout this notebook, for brevity's sake we will simply use the first author's name without a date when citing it going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sutskever used an input vocabulary size of 160,000 words and an output vocabulary size of 80,000 words, each word having an embedding of 1,000 parameters. This poses two challenges: memory use and runtime performance.\n",
    "\n",
    "Assuming we use float32 for the embedding parameters and the same vocabulary sizes as Sutskever, our embeddings would take up the following amount of memory:\n",
    "\n",
    "$$\n",
    "(160,000 + 80,000) \\cdot (1,000 \\text{ parameters}) \\cdot (\\frac{4 \\text{ bytes}}{1 \\text{ parameter}}) \\cdot (\\frac{1 \\text{ GB}}{1,024^3 \\text{ bytes}}) \\approx 0.9 \\text{ GB}\n",
    "$$\n",
    "\n",
    "While this does not seem like a substantial amount of memory, between the other parameters of the model, and the need to also fit the current training batch in GPU memory, this is somewhat more memory than we can afford on an 8GB GPU.\n",
    "\n",
    "To understand how large vocabulary sizes cause performance issue with softmax, lets reexamine how softmax works:\n",
    "\n",
    "$$\n",
    "\\sigma (z)_j = \\frac{e^{z_j}}{\\Sigma_{k=1}^{K} e^{z_{k}}} \\text{ where } z = \\theta^{T} a_{prev}\n",
    "$$\n",
    "\n",
    "Softmax requires two most unfortunate operations. First, it requires a giant sum, $\\Sigma_{k=1}^{K} e^{z_{k}}$, used to normalize logits so that they form a valid probability distribution summing to 1. With Sutskever's output vocabulary of 80,000, we have a sum over 80,000 numbers (i.e. $K=80,000$). Second, a gargantuan dot product between the output of the last hidden state, $a_{prev}$, and softmax layer's weight matrix, $\\theta$. With Sutskever using a batch size of 128, we have a dot product between a $(80,000 \\times 1,000)$ and a $(1,000 \\times 128)$ matrix. The sheer magnitude of this dot product turns out to have significant and detrimental implications for performance. See Chen et al. (2015) for a more extensive overview of these issues.\n",
    "\n",
    "Sutskever gets around these problems by using 8 GPUs to train their model, 4 of which are dedicated to the softmax layer. However, we only have access to a single 8GB GPU. Given that, the most expedient remedy to both the memory and runtime performance issues is to simply halve the vocabulary size. So instead of using 160,000 words for the input vocabulary, we use 80,000, and instead of 80,000 words for the output vocabulary, we use 40,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ENGLISH_VOCAB=80000\n",
    "MAX_FRENCH_VOCAB=40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem drastic, and it does have implications for translation accuracy, but it turns out, by way of Zipf's law (Zipf, 1949), that the top 10,000 most frequent words account for the vast majority of words used. We can see this by plotting how much of the training data is covered as the vocabulary size is increased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_source, word_counts_target = util.load_word_counts(cache_dir='./target', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_data_covered_by_vocab_size(word_counts):\n",
    "    word_counts_sorted = list(sorted(map(lambda x: x[1], \n",
    "                                         filter(lambda x: not x[0] in [util.SOS_IDX, util.EOS_IDX],\n",
    "                                                word_counts.items())), \n",
    "                                     reverse=True))\n",
    "    total = sum(word_counts_sorted)\n",
    "    frequencies = [count / total for count in word_counts_sorted]\n",
    "    cummulative = []\n",
    "    current_percent = 0\n",
    "    for percent in frequencies:\n",
    "        current_percent += percent\n",
    "        cummulative.append(current_percent)\n",
    "    return [0.0]+cummulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_coverage_source = percent_data_covered_by_vocab_size(word_counts_source)\n",
    "cummulative_coverage_target = percent_data_covered_by_vocab_size(word_counts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_word_counts(cummulative_coverage_source, cummulative_coverage_target):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('English')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.ylabel('Percentage Train Data Covered')\n",
    "    plt.semilogx(list(range(1, len(cummulative_coverage_source) + 1)), \n",
    "                 cummulative_coverage_source)\n",
    "    plt.axvline(80000, color='g', linestyle='--', label=\"80,000\")\n",
    "    plt.axvline(160000, color='r', linestyle='--', label=\"160,000\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(np.arange(0.0, 1.01, 0.1))\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('French')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.ylabel('Percentage Train Data Covered')\n",
    "    plt.semilogx(list(range(1, len(cummulative_coverage_target) + 1)), \n",
    "                 cummulative_coverage_target)\n",
    "    plt.axvline(40000, color='g', linestyle='--', label=\"40,000\")\n",
    "    plt.axvline(80000, color='r', linestyle='--', label=\"80,000\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.yticks(np.arange(0.0, 1.01, 0.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_word_counts(cummulative_coverage_source, cummulative_coverage_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with a vocabulary size of 10,000, well over 90% of the data is covered, and we begin to see diminishing returns in increasing the vocabulary size further. We can see below that the benefit of an English vocabulary size of 160,000 versus 80,000 amounts to less than a 1% difference in the amount of data out-of-vocabulary. As for the French vocabulary size, the difference between 80,000 versus 40,000 amounts to less than 1.5%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Train Coverage 80,000 words:  0.981\n",
      "English Train Coverage 160,000 words: 0.988\n",
      "Delta: 0.007\n",
      "\n",
      "French Train Coverage 40,000 words: 0.969\n",
      "French Train Coverage 80,000 words: 0.981\n",
      "Delta: 0.012\n"
     ]
    }
   ],
   "source": [
    "print(\"English Train Coverage 80,000 words:  %.3f\" % cummulative_coverage_source[80000])\n",
    "print(\"English Train Coverage 160,000 words: %.3f\" % cummulative_coverage_source[160000])\n",
    "print(\"Delta: %.3f\" % (cummulative_coverage_source[160000-1] - cummulative_coverage_source[80000]))\n",
    "print()\n",
    "print(\"French Train Coverage 40,000 words: %.3f\" % cummulative_coverage_target[40000])\n",
    "print(\"French Train Coverage 80,000 words: %.3f\" % cummulative_coverage_target[80000])\n",
    "print(\"Delta: %.3f\" % (cummulative_coverage_target[80000] - cummulative_coverage_target[40000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next issue is the batch size. Sutskever uses a batch size of 128. Unfortunately, for longer sentences, it becomes difficult to fit an entire training batch into GPU memory with this batch size. Consequently, we vary the batch size by sentence length to accommodate the memory constraints of a single GPU with 8GB of memory. For example, for a batch of sentences with lengths between 20 and 30 words, we use a batch size of 128, but for a batch of sentences between 100 and 120 words, we use a batch size of 32. Also, we cap sentence lengths at 120 words. Few samples are longer than this, so little training data is lost and some memory is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [20, 30, 40, 50, 60, 80, 100, 120]\n",
    "threshold_to_batch_size = {\n",
    "    20: 128,\n",
    "    30: 128,\n",
    "    40: 64,\n",
    "    50: 64,\n",
    "    60: 64,\n",
    "    80: 32,\n",
    "    100: 32,\n",
    "    120: 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these modifications, we can train Sutskever's model on a single 8GB GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSLM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSLM data set (Schwenk, 2014) provides a tokenized corpus of, among other things, English and French bitexts suitable for training a machine translation model. In particular, we use ep7_pc45, nc9, dev08_11, crawl, ccb2_pc30, and un2000_pc34 bitexts as our training data. Bitexts ntst1213 and ntst14 as our dev and test sets respectively. We use `<unk>`, `<s>` and `</s>` for the out-of-vocabulary (OOV), start-of-sentence (SOS), and end-of-sentence (EOS) tokens respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_source = util.load_source_word_index(cache_dir='./target', overwrite=False)\n",
    "word_index_target = util.load_target_word_index(cache_dir='./target', overwrite=False)\n",
    "index_word_source = {i: w for (w, i) in word_index_source.items()}\n",
    "index_word_target = {i: w for (w, i) in word_index_target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_dev, y_dev), (X_test, y_test) = util.load_data(MAX_ENGLISH_VOCAB, \n",
    "                                                                      MAX_FRENCH_VOCAB, \n",
    "                                                                      reverse_source=True, \n",
    "                                                                      cache_dir='./target', \n",
    "                                                                      overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, like Sutskever, we reverse the order of the English sentences, but not the French sentences, to improve the accuracy of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> . <unk> <unk> and Tom , Philippines the in arrival his after month one barely Just\n",
      "Tout juste un mois après son arrivée aux Philippines , Tom <unk> <unk> . </s>\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_word_source[idx] for idx in X_train[80006]]))\n",
    "print(' '.join([index_word_target[idx] for idx in y_train[80006]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are indexed according to their frequency such that words with higher frequency have indices lower than those with lower frequency. The exceptions being `<unk>`, `<s>`, and `</s>`, which for convenience, always have indices 1, 2, and 3. Also, in Keras, index 0 is reserved for the purpose of masking. For example, here are the 15 most frequent English words in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the', ',', 'of', '.', 'and', 'to', 'in', 'a', '(', ')', 'for', 'is', 'The', 'on', 'that'\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(\"'\" + index_word_source[i] + \"'\" for i in range(4,19)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the 15 least frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'50,008,325', 'accessaccessible', 'fifth-sixth', '273,375,000', '546,750,000', 'EUR3.3', 'EUR21.2', '1,467,500', 'AfghanTemplate.htm.', 'SUSU', '105-year', '67,179', 'MISC.30.', 'MISC.14-FCCC', 'assistance.16'\n"
     ]
    }
   ],
   "source": [
    "highest_index_source = sorted(index_word_source.keys(), reverse=True)[0]\n",
    "print(', '.join(\"'\" + index_word_source[highest_index_source - i] + \"'\" for i in range(0, 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing words in this way makes it trivial to change the vocabulary size without having to re-index all of the words in the vocabulary as we can simply replace any indices above our vocabulary size with the OOV token as we've done when loading the CSLM data above. For comparison, here are the 15 least frequent words given our vocabulary size `MAX_ENGLISH_VOCAB`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Inmate', 'Tanger', 'GOVERNANCE', 'symbolising', '2051', 'in-orbit', 'MPM', 'Plante', 'Mays', 'Guérin', 'dampness', 'Tetouan', 'Heerlen', 'Voted', 'Collar'\n"
     ]
    }
   ],
   "source": [
    "highest_index_source = max(max(tokenized_sentence) for tokenized_sentence in X_train)\n",
    "print(', '.join(\"'\" + index_word_source[highest_index_source - i] + \"'\" for i in range(0, 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much more word-like the least frequent words within the vocabulary size are versus the least common words overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional work needs to be performed on this data before it is ready for training the model. The model requires as input: encoder inputs and decoder inputs. It also requires the labels. The encoder inputs are effectively the source sentences. The decoder inputs are essentially the target sentences, except prefixed by the start-of-sentence token. The labels are the target sentences. Note that the offset between the decoder inputs and the labels is because the decoder is effectively a language model where we are trying to predict the next word in a sentence. So, for time step `t`, the decoder input at `t` is the word preceding the label at `t`. \n",
    "\n",
    "Also, since we will be working with batches of training data, and batches are provided via tensors, all samples in a batch must have the same length. To accommodate this, we pad samples in a batch with `0` so that they are all the same length. These 0 values will be masked during training so they will not bias the loss. So do we put our padding at the beginning or the end of our samples? We want to reduce as much as possible the distance between the output of the final word in the encoder and the first word of the decoder. If we insert padding in-between the two, then the encoder must learn how to carry information over this dead space and the decoder must learn to do likewise. We can avoid this problem by simply padding the encoder inputs at the beginning, and the decoder inputs/labels at the end. Now the output of the encoder will be generated right at the final word for each source sentence sample in the batch, and the inputs for decoder will start at the beginning of each target sentence in the batch.\n",
    "\n",
    "To accomplish the above, we use the the below `to_model_inputs_and_labels(..)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_model_inputs_and_labels(X, y):\n",
    "    X_padded_array = pad_sequences(X,\n",
    "                                   dtype='int32',\n",
    "                                   padding='pre',\n",
    "                                   truncating='post',\n",
    "                                   value=0)\n",
    "    y_padded_array = pad_sequences(y,\n",
    "                                   dtype='int32',\n",
    "                                   padding='post',\n",
    "                                   truncating='post',\n",
    "                                   value=0)\n",
    "    decoder_inputs = np.roll(y_padded_array, shift=1, axis=1)\n",
    "    decoder_inputs[:, 0] = util.SOS_IDX\n",
    "    decoder_inputs[decoder_inputs == util.EOS_IDX] = 0\n",
    "    return ((X_padded_array, decoder_inputs), y_padded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_and_labels_example = to_model_inputs_and_labels(X_train[80006:80008], y_train[80006:80008])\n",
    "((encoder_input_example, decoder_input_example), labels_example) = model_input_and_labels_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that samples for encoder inputs are padded on the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example encoder inputs:\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     3     7     1     1     8  9711     5  2904\n",
      "      4    10  2791   107   200  1461    58  9916  3131]\n",
      " [    3     7     1 20965  2701     1     6  6714     4 14827   425    29\n",
      "      8   540     6  2590     4    17    76  2154    57    41    71    17\n",
      "    505  3045    11    29    20  9093     4 65136    20]]\n"
     ]
    }
   ],
   "source": [
    "print(\"example encoder inputs:\")\n",
    "print(encoder_input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that samples for the decoder inputs are padded on the right and that the decoder inputs are the same as the labels except prefixed by the `<s>` token (which has index 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example decoder inputs:\n",
      "[[    2  1055  1133    18   379   157    59  1759    34  2867     5 10982\n",
      "      1     1     6     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [    2    27     1     9     1    27    25    48    88     9  1135  2129\n",
      "      4     7    76  4612    14   717    26     7   174     4     7   658\n",
      "     28    85     5   932    75 23991    11  8161     4    74  2189     5\n",
      "      7     1     8  1561 24207     1     6]]\n",
      "\n",
      "example decoder labels:\n",
      "[[ 1055  1133    18   379   157    59  1759    34  2867     5 10982     1\n",
      "      1     6     3     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [   27     1     9     1    27    25    48    88     9  1135  2129     4\n",
      "      7    76  4612    14   717    26     7   174     4     7   658    28\n",
      "     85     5   932    75 23991    11  8161     4    74  2189     5     7\n",
      "      1     8  1561 24207     1     6     3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"example decoder inputs:\")\n",
    "print(decoder_input_example)\n",
    "print()\n",
    "print(\"example decoder labels:\")\n",
    "print(labels_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, while Keras will mask the padding values to prevent biasing the loss that does not mean that the padding values come free of computational and memory costs. For this reason, we want to limit just how many padding values are in each batch. To that end, we group together sentences that are _similar_ in length to help reduce the amount of padding we need. Sutskever sped up training by a factor of two in this way. We have already defined the thresholds we will use above.\n",
    "\n",
    "We use a custom `Sequence` subclass, `BitextSequence`, which provides the appropriate inputs and labels for the model (i.e. it calls `to_model_inputs_and_labels(..)`), handles grouping together sentences of similar lengths, shuffling the data, and restoring the sequence, deterministically, to it's a appropriate state given the current epoch. The last capability is important for continuing training after an interruption (for example, after power loss). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitextSequence(Sequence):\n",
    "    def __init__(self, X, y, batch_size, thresholds=None, truncate=False, shuffle=False, seed=1):\n",
    "        if not shuffle:\n",
    "            thresholds = None\n",
    "            if isinstance(batch_size, dict):\n",
    "                batch_size = min(batch_size.values())\n",
    "                \n",
    "        if not shuffle and isinstance(batch_size, dict):\n",
    "            batch_size = min(batch_size.values())\n",
    "\n",
    "        if not thresholds:\n",
    "            self.thresholds = [max(map(lambda x, y: len(x)+len(y), X, y))]\n",
    "        else:\n",
    "            self.thresholds = list(sorted(thresholds))\n",
    "            \n",
    "        if isinstance(batch_size, dict):\n",
    "            self.threshold_to_batch_size = batch_size\n",
    "        else:\n",
    "            self.threshold_to_batch_size = {threshold: batch_size for threshold in self.thresholds}\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.truncate = truncate\n",
    "        self.seed = seed\n",
    "        \n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.threshold_to_ids = util.build_threshold_to_ids(\n",
    "            self._X, \n",
    "            self._y, \n",
    "            self.thresholds, \n",
    "            truncate=self.truncate)\n",
    "        batch_ids = []\n",
    "        for threshold in sorted(self.threshold_to_ids.keys()):\n",
    "            ids = self.threshold_to_ids[threshold]\n",
    "            batch_size = self.threshold_to_batch_size[threshold]\n",
    "            for i in range(int(np.ceil(len(ids)/float(batch_size)))):\n",
    "                batch_ids.append((threshold, i))        \n",
    "        self.batch_ids = batch_ids\n",
    "        self._random_state = np.random.RandomState(seed=self.seed)\n",
    "        self._shuffle_data()\n",
    "        \n",
    "    def _shuffle_data(self):\n",
    "        if self.shuffle:\n",
    "            for threshold in self.thresholds:\n",
    "                self._random_state.shuffle(self.threshold_to_ids[threshold])\n",
    "            self._random_state.shuffle(self.batch_ids)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print(\"Iteration complete, shuffling data\")\n",
    "        self._shuffle_data()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.batch_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        (threshold, i) = self.batch_ids[index]\n",
    "        batch_size = self.threshold_to_batch_size[threshold]\n",
    "\n",
    "        # get indices for data\n",
    "        indices = self.threshold_to_ids[threshold][i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # get inputs and labels for batch\n",
    "        encoder_inputs_batch = []\n",
    "        labels_batch = []\n",
    "        for i in indices:\n",
    "            encoder_inputs_batch.append(self._X[i])\n",
    "            labels_batch.append(self._y[i])\n",
    "\n",
    "        # decoder batch\n",
    "        (encoder_inputs_batch, decoder_inputs_batch), labels_batch =  to_model_inputs_and_labels(\n",
    "            encoder_inputs_batch, labels_batch)\n",
    "\n",
    "        return (encoder_inputs_batch, decoder_inputs_batch), labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bitext_sequence = BitextSequence(X_train, y_train,\n",
    "                                       batch_size=threshold_to_batch_size,\n",
    "                                       thresholds=thresholds,\n",
    "                                       truncate=True,\n",
    "                                       shuffle=True)\n",
    "dev_bitext_sequence = BitextSequence(X_dev, y_dev, \n",
    "                                     batch_size=threshold_to_batch_size,\n",
    "                                     thresholds=thresholds,\n",
    "                                     truncate=True,\n",
    "                                     shuffle=False)\n",
    "test_bitext_sequence = BitextSequence(X_test, y_test, \n",
    "                                      batch_size=threshold_to_batch_size,\n",
    "                                      thresholds=thresholds,\n",
    "                                      truncate=False,\n",
    "                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the 20th batch from the train sequence. Notice that the output is structured the same as the output of `to_model_inputs_and_labels(..)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 20th batch: [((128, 20), (128, 20)), (128, 20)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[    0,     0,     0, ...,     5,   137,   178],\n",
       "         [    0,     0,     0, ...,  7505,  7389,    16],\n",
       "         [    0,     0,     0, ...,    25,  3158, 47438],\n",
       "         ...,\n",
       "         [    0,     0,     0, ...,     4,     9,  4584],\n",
       "         [    0,     0,     0, ...,    46,     5, 19440],\n",
       "         [    0,     0,     0, ...,    13,   305,    12]], dtype=int32),\n",
       "  array([[    2,   217,   767, ...,     0,     0,     0],\n",
       "         [    2,    13, 15188, ...,     0,     0,     0],\n",
       "         [    2,   295,  1841, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2,  6429,    26, ...,     0,     0,     0],\n",
       "         [    2, 23154,     5, ...,     0,     0,     0],\n",
       "         [    2,   341,    17, ...,     0,     0,     0]], dtype=int32)),\n",
       " array([[  217,   767,    23, ...,     0,     0,     0],\n",
       "        [   13, 15188,   445, ...,     0,     0,     0],\n",
       "        [  295,  1841,     1, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 6429,    26,    13, ...,     0,     0,     0],\n",
       "        [23154,     5,   354, ...,     0,     0,     0],\n",
       "        [  341,    17,  1055, ...,     0,     0,     0]], dtype=int32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of 20th batch:\", [tuple([x.shape for x in train_bitext_sequence[20][0]]), \n",
    "                               train_bitext_sequence[20][1].shape])\n",
    "train_bitext_sequence[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, observe that the sequence supports shuffling but is still deterministic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration complete, shuffling data\n",
      "Iteration complete, shuffling data\n"
     ]
    }
   ],
   "source": [
    "train_bitext_sequence.reset()\n",
    "the_20th_train_batch_epoch_1 = train_bitext_sequence[20]\n",
    "train_bitext_sequence.on_epoch_end()\n",
    "the_20th_train_batch_epoch_2 = train_bitext_sequence[20]\n",
    "\n",
    "# we could get incredibly lucky here, and despite shuffling, the 20th batch \n",
    "# just happens to be the same, but lets pretend that won't happen \n",
    "assert not (np.array_equal(the_20th_train_batch_epoch_1[0][0], the_20th_train_batch_epoch_2[0][0])\n",
    "           and np.array_equal(the_20th_train_batch_epoch_1[0][1], the_20th_train_batch_epoch_2[0][1])\n",
    "           and np.array_equal(the_20th_train_batch_epoch_1[1], the_20th_train_batch_epoch_2[1]))\n",
    "\n",
    "train_bitext_sequence.reset()\n",
    "train_bitext_sequence.on_epoch_end()\n",
    "\n",
    "the_20th_train_batch_epoch_2_after_reset = train_bitext_sequence[20]\n",
    "\n",
    "assert (np.array_equal(the_20th_train_batch_epoch_2_after_reset[0][0], the_20th_train_batch_epoch_2[0][0])\n",
    "        and np.array_equal(the_20th_train_batch_epoch_2_after_reset[0][1], the_20th_train_batch_epoch_2[0][1])\n",
    "        and np.array_equal(the_20th_train_batch_epoch_2_after_reset[1], the_20th_train_batch_epoch_2[1]))\n",
    "\n",
    "train_bitext_sequence.reset() # restore the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants we will need\n",
    "MODEL_WEIGHTS_DIRECTORY='./model_checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output at each time-step is a probability distribution over all words in the target vocabulary, the cost function is unsurprisingly cross entropy loss. As opposed to using one-hot vectors for our labels though, which can take up a considerable amount of memory, we use a sparse cross entropy loss which enables us to just provide the word indices for the labels. Now Keras provides a `keras.losses.sparse_categorical_crossentropy(..)` function, which looks a lot like what one should use, but any such use will end in dismay and frustration when using the TensorFlow backend. The issue is that `keras.losses.sparse_categorical_crossentropy(..)` expects a normalized probability distribution (as would be produced from a softmax layer), but the `tf.nn.sparse_softmax_cross_entropy_with_logits(..)` function that it wraps expects unnormalized logits. Keras does some magic to turn the given probability distribution into something suitable for TensorFlow's `sparse_softmax_cross_entropy_with_logits(..)`, but not without consequences for the model's performance.\n",
    "\n",
    "The issue here is that Keras's loss functions take as input the output of the model, but TensorFlow expects logits for performance an numeric stability reasons. See TensorFlow issue [#2462](https://github.com/tensorflow/tensorflow/issues/2462) for details. To resolve this, we make a `keras.Model` instance specifically for training that produces logits, instead of probabilities, as output and then combine it with the below custom loss, `sparse_cross_entropy(..)`, that appropriately wraps TensorFlow's `sparse_softmax_cross_entropy_with_logits(..)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note about the loss function: its return value should have the same shape as the labels (`y_true`). Keras decorates the provided loss function to handle masking and weighting. Returning a scalar, as one might expect to do, will lead to confusing results. See discussion in TensorFlow issue [#17150](https://github.com/tensorflow/tensorflow/issues/17150)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of sparse_cross_entropy(..)\n",
    "with tf.Session() as session:\n",
    "    logits = np.random.random((20, 5, 1000))\n",
    "    labels = np.random.randint(1000, size=(20, 5))\n",
    "    loss_result = session.run(sparse_cross_entropy(labels, logits))\n",
    "    assert labels.shape == loss_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_train_encoder_decoder_models(..)` function returns 3 instances of `keras.Model`. The first is the model for use in training, the second and third are the encoder and decoder models used for inference. \n",
    "\n",
    "The training model takes batches of source and target sequences, as provided by an instance of `BitextSequence` or a call to `to_model_inputs_and_labels(..)` and outputs logits for every time-step of the target sequence.\n",
    "\n",
    "The encoder model takes as input a batch of source sequences and outputs thought vectors for each LSTM layer. The decoder model takes thought vectors and previous word input for the target, and outputs the top 100 words next possible words for every sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_results(logits, beam_size):\n",
    "    log_prob = tf.nn.log_softmax(logits, axis=-1)\n",
    "    indices = tf.constant([[util.OOV_IDX]], dtype='int32')\n",
    "    updates = tf.constant([-100000.0], dtype='float32')\n",
    "    scatter = tf.scatter_nd(indices, updates, tf.shape(logits)[-1:])\n",
    "    adjusted_log_prob = tf.add(log_prob, scatter)\n",
    "    [values, indices] = tf.nn.top_k(adjusted_log_prob, k=beam_size, sorted=True)\n",
    "    return [tf.identity(values), tf.identity(indices)]\n",
    "\n",
    "def compute_top_results_shape(input_shape, beam_size):\n",
    "    shape = tuple(list(input_shape[:-1]) + [beam_size])\n",
    "    return [shape, shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_encoder_decoder_models(units_rnn_layer=1000, word_vec_dim=1000, beam_size=100):\n",
    "    number_rnn_layers = 4\n",
    "    \n",
    "    # inputs\n",
    "    encoder_inputs = Input(shape=(None,), dtype='int32', name='encoder_inputs')\n",
    "    decoder_inputs = Input(shape=(None,), dtype='int32', name='decoder_inputs')\n",
    "    \n",
    "    decoder_initial_state_inputs = []\n",
    "    for i in range(number_rnn_layers):\n",
    "        decoder_initial_state_inputs.append(Input(shape=(units_rnn_layer,), name=\"initial_state_h_%s\" % i))\n",
    "        decoder_initial_state_inputs.append(Input(shape=(units_rnn_layer,), name=\"initial_state_c_%s\" % i))\n",
    "        \n",
    "    # param initializer\n",
    "    # It is unclear if Sutskever et al. intended this initialization only for LSTM layers\n",
    "    # or for the entire network. I'm assuming the latter.\n",
    "    initializer = RandomUniform(minval=-0.08, maxval=0.08)\n",
    "    \n",
    "    # encoder layers\n",
    "    source_embedding = Embedding(MAX_ENGLISH_VOCAB+1, \n",
    "                                 word_vec_dim, \n",
    "                                 mask_zero=True, \n",
    "                                 name=\"source_embedding\", \n",
    "                                 embeddings_initializer=initializer)\n",
    "    \n",
    "    encoder_rnn_layers = []\n",
    "    for i in range(number_rnn_layers-1):\n",
    "        encoder_rnn_layers.append(LSTM(units_rnn_layer, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       name=\"encoder_rnn_%s\" % i,\n",
    "                                       kernel_initializer=initializer))\n",
    "    encoder_rnn_layers.append(LSTM(units_rnn_layer, \n",
    "                                   return_state=True, \n",
    "                                   name=\"encoder_rnn_%s\" % (number_rnn_layers-1,),\n",
    "                                   kernel_initializer=initializer))\n",
    "    \n",
    "    \n",
    "    # decoder layers\n",
    "    target_embedding = Embedding(MAX_FRENCH_VOCAB+1, \n",
    "                                 word_vec_dim, \n",
    "                                 mask_zero=True, \n",
    "                                 name=\"target_embedding\",\n",
    "                                 embeddings_initializer=initializer)\n",
    "    \n",
    "    decoder_rnn_layers = []\n",
    "    for i in range(number_rnn_layers):\n",
    "        decoder_rnn_layers.append(LSTM(units_rnn_layer, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       name=\"decoder_rnn_%s\" % i,\n",
    "                                       kernel_initializer=initializer))\n",
    "    \n",
    "    vocab_size = MAX_FRENCH_VOCAB+1\n",
    "    decoder_dense = TimeDistributed(Dense(vocab_size, \n",
    "                                          activation='linear', \n",
    "                                          name=\"decoder_projection\", \n",
    "                                          kernel_initializer=initializer))\n",
    "    \n",
    "    top_results_layer = Lambda(lambda x: compute_top_results(x, beam_size=beam_size),\n",
    "                               output_shape=lambda input_shape: compute_top_results_shape(input_shape, beam_size=beam_size))\n",
    "    \n",
    "    # build train model    \n",
    "    x = source_embedding(encoder_inputs)\n",
    "    \n",
    "    encoder_states = []\n",
    "    for encoder_rnn in encoder_rnn_layers:\n",
    "        x, state_h, state_c = encoder_rnn(x)\n",
    "        encoder_states.append(state_h)\n",
    "        encoder_states.append(state_c)\n",
    "    \n",
    "    x = target_embedding(decoder_inputs)\n",
    "    grouped_encoder_states = [encoder_states[i:i + 2] for i in range(0, len(encoder_states), 2)]\n",
    "    for (decoder_rnn, initial_state) in zip(decoder_rnn_layers, grouped_encoder_states):\n",
    "        x, _, _ = decoder_rnn(x, initial_state=initial_state)\n",
    "    x = decoder_dense(x)\n",
    "    \n",
    "    train_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=x)\n",
    "    sgd_optimizer = SGD(lr=0.7, clipnorm=5.)\n",
    "    \n",
    "    # This bit of hoodoo is thanks to https://github.com/tensorflow/tensorflow/issues/17150. It works around\n",
    "    # a bug where Keras cannot figure out the proper output shape.\n",
    "    decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
    "    train_model.compile(optimizer=sgd_optimizer, \n",
    "                        loss=sparse_cross_entropy, \n",
    "                        target_tensors=[decoder_target])\n",
    "    \n",
    "    # build encoder model\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "    # build decoder model\n",
    "    x = target_embedding(decoder_inputs)\n",
    "    grouped_decoder_initial_state_inputs = [decoder_initial_state_inputs[i:i + 2] \n",
    "                                            for i in range(0, len(decoder_initial_state_inputs), 2)]\n",
    "    decoder_outputs = []\n",
    "    for (decoder_rnn, initial_state) in zip(decoder_rnn_layers, grouped_decoder_initial_state_inputs):\n",
    "        x, state_h, state_c = decoder_rnn(x, initial_state=initial_state)\n",
    "        decoder_outputs.append(state_h)\n",
    "        decoder_outputs.append(state_c)\n",
    "    x = decoder_dense(x)\n",
    "    top_values_indices = top_results_layer(x)\n",
    "    \n",
    "    decoder_model = Model(inputs=[decoder_inputs] + decoder_initial_state_inputs, outputs=top_values_indices + decoder_outputs)\n",
    "\n",
    "    return (train_model, encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During development, we inevitably make numerous tweaks to the model and need to reload it occasionally. To facilitate this, we need to clean up any previously created models and free GPU memory before re-instantiating the models. Accomplishing this is a bit finicky and unreliable, but short of sacrificing a goat, I don't know what else to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "source_embedding (Embedding)    (None, None, 1000)   80001000    encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "target_embedding (Embedding)    (None, None, 1000)   40001000    decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_rnn_0 (LSTM)            [(None, None, 1000), 8004000     source_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "decoder_rnn_0 (LSTM)            [(None, None, 1000), 8004000     target_embedding[0][0]           \n",
      "                                                                 encoder_rnn_0[0][1]              \n",
      "                                                                 encoder_rnn_0[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_rnn_1 (LSTM)            [(None, None, 1000), 8004000     encoder_rnn_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_rnn_1 (LSTM)            [(None, None, 1000), 8004000     decoder_rnn_0[0][0]              \n",
      "                                                                 encoder_rnn_1[0][1]              \n",
      "                                                                 encoder_rnn_1[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_rnn_2 (LSTM)            [(None, None, 1000), 8004000     encoder_rnn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_rnn_2 (LSTM)            [(None, None, 1000), 8004000     decoder_rnn_1[0][0]              \n",
      "                                                                 encoder_rnn_2[0][1]              \n",
      "                                                                 encoder_rnn_2[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_rnn_3 (LSTM)            [(None, 1000), (None 8004000     encoder_rnn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_rnn_3 (LSTM)            [(None, None, 1000), 8004000     decoder_rnn_2[0][0]              \n",
      "                                                                 encoder_rnn_3[0][1]              \n",
      "                                                                 encoder_rnn_3[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 40001)  40041001    decoder_rnn_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 224,075,001\n",
      "Trainable params: 224,075,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del inference_helper\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del encoder_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del decoder_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "model, encoder_model, decoder_model = build_train_encoder_decoder_models()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training model takes batches of source and target sequences, as provided by an instance of `BitextSequence`, and outputs logits for every time-step of the target sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model output shape: (128, 20, 40001)\n",
      "Train model cost output: 10.601037502288818\n"
     ]
    }
   ],
   "source": [
    "((encoder_input_samples, decoder_input_samples), labels) = train_bitext_sequence[20]\n",
    "# model.evaluate(x=[encoder_input_samples, decoder_input_samples], y=labels)\n",
    "train_model_output = model.predict([encoder_input_samples, decoder_input_samples])\n",
    "print(\"Train model output shape:\", train_model_output.shape)\n",
    "train_cost_output = model.evaluate(x=[encoder_input_samples, decoder_input_samples], y=labels, verbose=0)\n",
    "print(\"Train model cost output:\", train_cost_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder model takes as input a batch of source sequences and outputs each LSTM layer's output and state (these are the thought vectors). The decoder model takes thought vectors and previous word input for the target, and outputs the top 100 words next possible words for every sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encoder outputs: 8\n",
      "Shape of each encoder output: (1, 1000)\n",
      "Shape of top log probabilities: (1, 1, 100)\n",
      "Shape of top word indices: (1, 1, 100)\n",
      "Top 10 word indices: [30956  6931 12876  7775  8401 39762  9886  4403 16279 14601]\n",
      "Number decoder outputs for next word: 8\n",
      "Shape of each decoder output for next word: (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "((encoder_input_samples, decoder_input_samples), labels) = train_bitext_sequence[20]\n",
    "encoder_input_sample = encoder_input_samples[:1]\n",
    "encoder_outputs_states = encoder_model.predict(encoder_input_sample)\n",
    "\n",
    "assert 1 == len(set([output.shape for output in encoder_outputs_states]))\n",
    "\n",
    "print(\"Number of encoder outputs:\", len(encoder_outputs_states))\n",
    "print(\"Shape of each encoder output:\", encoder_outputs_states[0].shape)\n",
    "\n",
    "top_log_probs, top_word_indices, *decoder_outputs_states = decoder_model.predict(\n",
    "    [np.array([util.SOS_IDX], dtype='int32')]+encoder_outputs_states)\n",
    "\n",
    "assert 1 == len(set([output.shape for output in decoder_outputs_states]))\n",
    "\n",
    "print(\"Shape of top log probabilities:\", top_log_probs.shape)\n",
    "print(\"Shape of top word indices:\", top_word_indices.shape)\n",
    "print(\"Top 10 word indices:\", top_word_indices[0,0,:10])\n",
    "print(\"Number decoder outputs for next word:\", len(decoder_outputs_states))\n",
    "print(\"Shape of each decoder output for next word:\", decoder_outputs_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a checkpoint callback to save the model weights at the end of each epoch. This is useful for saving progress in case training gets interrupted, and also permits us to analyze the model at different points during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_filename_pattern = util.get_weight_filename_pattern(MODEL_WEIGHTS_DIRECTORY)\n",
    "checkpoint_callback = ModelCheckpoint(model_checkpoint_filename_pattern, period=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take multiple days to train the model for one iteration. Ideally, we'd like to trigger our end-of-epoch callbacks, like the one for saving weights, more frequently than that. To accomplish this, we will redefine an epoch to be a partial iteration. This will cause Keras to trigger the end-of-epoch callbacks multiple times each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_PER_ITERATION = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Sutskever, we adjust the learning rate down in later iterations during training. We start with a learning rate of `0.7` and halve the learning every half iteration after the fifth. We use a `LearningRateScheduler` callback to accomplish this, keeping in mind we are using multiple epochs per iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_update_learning_rate_fn(start_lr):\n",
    "    epoch_of_5th_iteration = EPOCHS_PER_ITERATION * 5\n",
    "    epochs_in_half_iteration = EPOCHS_PER_ITERATION // 2\n",
    "    def update_learning_rate(epoch_index, lr):\n",
    "        # NOTE: We do not want to half the lr parameter. Doing so has a bad interaction with\n",
    "        # with training interruption. For example, on iteration 6, we should have a lerning\n",
    "        # rate of 0.175 (assuming start_lr == 0.7). However, if at the beginning of iteration\n",
    "        # 6 we reload the notebook and pick up training where we left off, if we simply half\n",
    "        # lr we will have a learning rate of 0.35! This is because, after restart, \n",
    "        # update_learning_rate will only be called on epochs >= the epoch training is \n",
    "        # continued from. To get around this, we rely exclusively on the epoch_index and \n",
    "        # start_lr for determing the current learning rate.\n",
    "        number_times_to_halve = max(0, (epoch_index - epoch_of_5th_iteration) // epochs_in_half_iteration)\n",
    "        return start_lr / (2**number_times_to_halve)\n",
    "    return update_learning_rate\n",
    "update_learning_rate = create_update_learning_rate_fn(0.7)\n",
    "lr_callback = LearningRateScheduler(update_learning_rate, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate after 5.0 iterations: 0.7\n",
      "Learning rate after 5.5 iterations: 0.35\n",
      "Learning rate after 6.0 iterations: 0.175\n"
     ]
    }
   ],
   "source": [
    "learning_rate_iteration_5_0 = update_learning_rate(int(5.0 * EPOCHS_PER_ITERATION), None)\n",
    "learning_rate_iteration_5_5 = update_learning_rate(int(5.5 * EPOCHS_PER_ITERATION), None)\n",
    "learning_rate_iteration_6_0 = update_learning_rate(int(6.0 * EPOCHS_PER_ITERATION), None)\n",
    "assert 0.7 == learning_rate_iteration_5_0\n",
    "assert 0.35 == learning_rate_iteration_5_5\n",
    "assert 0.175 == learning_rate_iteration_6_0\n",
    "print(\"Learning rate after 5.0 iterations:\", learning_rate_iteration_5_0)\n",
    "print(\"Learning rate after 5.5 iterations:\", learning_rate_iteration_5_5)\n",
    "print(\"Learning rate after 6.0 iterations:\", learning_rate_iteration_6_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to translate between `EPOCHS_PER_ITERATION` and the epochs `BitextSequence` instances expect, we wrap them with an instance of `EpochAsPartialIterationSequenceWrapper`. This wrapper will handle making calls to `on_epoch_end(..)` on our `BitextSequence` instances as appropriate and will allow us to pick up approximately where we left off if training is interrupted in the middle of an iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number batches in train_bitext_sequence: 146576\n",
      "Number batches in train_bitext_sequence_wrapped: 3664\n",
      "Iteration complete, shuffling data\n"
     ]
    }
   ],
   "source": [
    "train_bitext_sequence_wrapped = util.EpochAsPartialIterationSequenceWrapper(train_bitext_sequence, \n",
    "                                                                            EPOCHS_PER_ITERATION)\n",
    "print(\"Number batches in train_bitext_sequence:\", len(train_bitext_sequence))\n",
    "print(\"Number batches in train_bitext_sequence_wrapped:\", len(train_bitext_sequence_wrapped))\n",
    "\n",
    "# Note that `on_epoch_end()` must be called `EPOCHS_PER_ITERATION` times on the \n",
    "# `EpochAsPartialIterationSequenceWrapper` instance before `on_epoch_end()` is called on \n",
    "# the underlying sequence causing it to be shuffled.\n",
    "for _ in range(EPOCHS_PER_ITERATION):\n",
    "    train_bitext_sequence_wrapped.on_epoch_end()\n",
    "    \n",
    "del train_bitext_sequence_wrapped # we'll create a new wrapper during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we load the last saved weights and set `current_epoch` to the appropriate epoch index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from ./model_checkpoints/weights.300-1.9027-1.4531.h5\n"
     ]
    }
   ],
   "source": [
    "current_epoch=0\n",
    "# load last saved weights if present\n",
    "weight_file_and_epoch = util.latest_epoch_and_weights_file(MODEL_WEIGHTS_DIRECTORY)\n",
    "if weight_file_and_epoch:\n",
    "    weights_file, current_epoch = weight_file_and_epoch\n",
    "    print(\"Loading weights from %s\" % weights_file)\n",
    "    model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin training. Note that it took 23 days to train this model, so the next cell can take a while to run if starting from scratch. However, due to our efforts above, it should be possible to interrupt training with minimal time lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_epoch < int(7.5*EPOCHS_PER_ITERATION):\n",
    "    train_bitext_sequence_wrapped = util.EpochAsPartialIterationSequenceWrapper(\n",
    "    train_bitext_sequence,\n",
    "    EPOCHS_PER_ITERATION)\n",
    "    train_bitext_sequence_wrapped.switch_to_epoch(current_epoch)\n",
    "\n",
    "    model.fit_generator(\n",
    "        generator=train_bitext_sequence_wrapped,\n",
    "        validation_data=dev_bitext_sequence,\n",
    "        steps_per_epoch=train_bitext_sequence_wrapped.steps_per_epoch,\n",
    "        shuffle=False, # our sequence type does this for us\n",
    "        initial_epoch=current_epoch,\n",
    "        epochs=int(EPOCHS_PER_ITERATION*7.5),\n",
    "        workers=2,\n",
    "        callbacks=[checkpoint_callback, lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the loss on our validation and training sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(model_weights_directory):\n",
    "    model_props = [props for (f, props) in util.list_all_weight_files_and_props(model_weights_directory)]\n",
    "    model_props_by_epoch = sorted(model_props, key=lambda x: x[0])\n",
    "    (epoch, val_loss, loss) = zip(*model_props_by_epoch)\n",
    "    epoch = [epoch / 40.0 for epoch in epoch]\n",
    "    plt.plot(epoch, loss, label='train loss')\n",
    "    plt.plot(epoch, val_loss, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Train and Validation Loss per Epoch\")\n",
    "    plt.xlim(0, epoch[-1])\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(MODEL_WEIGHTS_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the anomaly between iterations 0.5 and 1 where the loss appears to shoot up. This is due to error in `sparse_cross_entropy(..)` above. Originally, the function returned a scalar, instead of a tensor with the same dimensions as the labels, which led Keras to incorrectly adjust the loss based on the padding. Consequently, the loss was lower than it should have been until the bug was corrected around iteration 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a beam search to generate translations. There are a number of ways to implement beam search, in particular around the stop condition, and the specifics of how it was implemented in Sutskever are unclear. It turns out the way we have implemented it below is similar to Bahdanau et al. (2014) in that we shrink the beam length every time a candidate translation is completed until the beam length becomes zero. According to Huang et al. (2017), this approach is suboptimal, but such is life.\n",
    "\n",
    "We use 0.7 for the sentence length normalization constant. Without normalization, beam search tends to prefer succinct, but perhaps less accurate, translations over longer, perhaps more natural, translations. If we set this constant to 0, there would be no normalization, if we set this constant to 1, the probability of a candidate translation would be divided by the number of words in the sentence--effectively eliminating sentence length as a consideration. Everything in-between, such as 0.7 as we have chosen, is a compromise between the two.\n",
    "\n",
    "Also, another note concerning the below implementation: it is incredibly and unbelievably slow. Probably don't reuse this code. I'm sure better implementations exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@do_profile(follow=[])\n",
    "def beam_search(encoder_output_states, decoder_model, maxlen, \n",
    "                sos_token_index, eos_token_index, oov_token_index=util.OOV_IDX, \n",
    "                black_list=None,\n",
    "                beam_length=10, alpha=0.7):\n",
    "    \n",
    "    if black_list is None:\n",
    "        black_list = [oov_token_index]\n",
    "    \n",
    "    completed_candidates = []\n",
    "    \n",
    "    previous_state_arrays = encoder_output_states[:]\n",
    "    decoder_inputs = np.array([[sos_token_index]])\n",
    "    candidate_log_prob_sum = np.array([[0]])\n",
    "    \n",
    "    while len(completed_candidates) < beam_length:\n",
    "        shrunk_beam_length = beam_length - len(completed_candidates)\n",
    "        \n",
    "        previous_token_idxs = decoder_inputs[:,-1:]\n",
    "        top_log_probs, top_indices, *previous_state_arrays = decoder_model.predict([previous_token_idxs] + previous_state_arrays)\n",
    "        \n",
    "        # remove uneeded time dimension (since we are only looking one word at a time)\n",
    "        top_log_probs = np.squeeze(top_log_probs, axis=1)\n",
    "        top_indices = np.squeeze(top_indices, axis=1)\n",
    "        \n",
    "        # sort log probs and word indices by log prob\n",
    "        sorted_indices = np.argsort(top_log_probs, axis=-1)\n",
    "        top_log_probs = top_log_probs[np.arange(np.shape(top_log_probs)[0])[:,np.newaxis], sorted_indices]\n",
    "        top_indices = top_indices[np.arange(np.shape(top_log_probs)[0])[:,np.newaxis], sorted_indices]\n",
    "\n",
    "        # for each candidate, calc the sum of log probabilities for potential next words        \n",
    "        candidate_top_choices_indices = top_indices[:, -shrunk_beam_length:]\n",
    "        candidate_top_choices = top_log_probs[:, -shrunk_beam_length:]\n",
    "        candidate_top_choices = np.add(candidate_top_choices, candidate_log_prob_sum, candidate_top_choices)\n",
    "            \n",
    "        # get indices of next top word choices\n",
    "        indices_1d = np.argpartition(candidate_top_choices, -shrunk_beam_length, axis=None)[-shrunk_beam_length:]\n",
    "        candidate_idxs, top_choices_idxs = np.unravel_index(indices_1d, candidate_top_choices.shape)\n",
    "        \n",
    "        # map top_choices_idxs back to the corresponding word indices\n",
    "        word_idxs = candidate_top_choices_indices[candidate_idxs, top_choices_idxs]\n",
    "        \n",
    "        # update previous state arrays for top candidates\n",
    "        previous_state_arrays = [prev[candidate_idxs,:] for prev in previous_state_arrays]\n",
    "        \n",
    "        # update decoder inputs for next iteration\n",
    "        decoder_inputs = np.append(decoder_inputs[candidate_idxs, :], \n",
    "                                   np.array(word_idxs).reshape((len(word_idxs), 1)),\n",
    "                                   axis=1)\n",
    "        \n",
    "        # update candidate prob based on top candidates\n",
    "        candidate_log_prob_sum = candidate_top_choices[candidate_idxs, top_choices_idxs].reshape(len(candidate_idxs), 1)\n",
    "        \n",
    "        # extract any completed candidates\n",
    "        incomplete_candidates = []\n",
    "        for candidate_idx in range(len(decoder_inputs)):\n",
    "            if decoder_inputs[candidate_idx][-1] == eos_token_index or decoder_inputs.shape[1] > maxlen:\n",
    "                completed_candidates.append((candidate_log_prob_sum[candidate_idx], decoder_inputs[candidate_idx]))\n",
    "            else:\n",
    "                incomplete_candidates.append(candidate_idx)\n",
    "        \n",
    "        # update values accounting for completed candidates\n",
    "        decoder_inputs = decoder_inputs[incomplete_candidates]\n",
    "        candidate_log_prob_sum = candidate_log_prob_sum[incomplete_candidates]\n",
    "        previous_state_arrays = [prev[incomplete_candidates,:] for prev in previous_state_arrays]\n",
    "    \n",
    "    # return candidate with highest probability accounting for sentence length normalization\n",
    "    def normalize_prob_by_sentence_length(completed_candidate):\n",
    "        prob, tokens = completed_candidate\n",
    "        return prob / (len(tokens)**(alpha))\n",
    "    \n",
    "    sorted_completed_candidates = sorted(completed_candidates, \n",
    "                                         reverse=True, \n",
    "                                         key=normalize_prob_by_sentence_length)\n",
    "    output_indices = sorted_completed_candidates[0]\n",
    "    \n",
    "    return output_indices[1][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `InferenceHelper` class takes care of the ceremony involved in translating texts using a beam search. You can translate source texts to target texts, source sequences to target sequences, source sequences to thought vectors, and thought vectors to target sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_valid_thought_vector(thought_vector):\n",
    "    assert len(thought_vector) == 8, \"Expected 8 entries in thought vector but found %s\" % len(thought_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_states_to_thought_vectors(encoder_states):\n",
    "    (samples, units) = encoder_states[0].shape\n",
    "    thought_vectors = [[np.reshape(state, (1,units)) for state in thought_vector] \n",
    "                       for thought_vector in zip(*encoder_states)]\n",
    "    assert len(thought_vectors) == samples\n",
    "    return thought_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_tokenizer(word_index, vocab_size):\n",
    "    new_word_index = {w: i for (w, i) in word_index.items() if i < vocab_size+1}\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, char_level=False, oov_token=util.OOV_TOKEN)\n",
    "    tokenizer.word_index = new_word_index\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceHelper:\n",
    "    def __init__(self, \n",
    "                 encoder_model, decoder_model, \n",
    "                 source_word_index, target_word_index,\n",
    "                 source_vocab_size, target_vocab_size,\n",
    "                 source_lang, target_lang):\n",
    "        \n",
    "        self.target_tokenizer = build_keras_tokenizer(target_word_index, target_vocab_size)\n",
    "        self.source_tokenizer = build_keras_tokenizer(source_word_index, source_vocab_size)\n",
    "        \n",
    "        self.target_lang = target_lang\n",
    "        self.source_lang = source_lang\n",
    "        \n",
    "        self.encoder_model = encoder_model\n",
    "        self.decoder_model = decoder_model\n",
    "        self.target_index_to_word = {i: w for (w, i) in self.target_tokenizer.word_index.items()}\n",
    "        self.source_index_to_word = {i: w for (w, i) in self.source_tokenizer.word_index.items()}\n",
    "        \n",
    "    def _sequences_to_texts(self, index_to_word, lang, sequences):\n",
    "        with MosesDetokenizer(lang=lang) as detokenize:\n",
    "            texts = ([index_to_word[i] for i in sequence if i != 0 and i != util.EOS_IDX] for sequence in sequences)\n",
    "            return list(map(detokenize, texts))\n",
    "    \n",
    "    def _texts_to_sequences(self, keras_tokenizer, lang, texts):\n",
    "        with MosesTokenizer(lang=lang) as tokenize:\n",
    "            tokens_lists = list(map(lambda x: x + [util.EOS_TOKEN], map(tokenize, texts)))\n",
    "            sequences = keras_tokenizer.texts_to_sequences(tokens_lists)\n",
    "            return sequences\n",
    "\n",
    "    def sequences_to_target_texts(self, sequences):\n",
    "        return self._sequences_to_texts(self.target_index_to_word, self.target_lang, sequences)\n",
    "    \n",
    "    def sequences_to_source_texts(self, sequences):\n",
    "        return self._sequences_to_texts(self.source_index_to_word, self.source_lang, sequences)\n",
    "    \n",
    "    def sequences_to_target_tokens(self, sequences):\n",
    "        return [[self.target_index_to_word[i] for i in sequence if i != 0 and i != util.EOS_IDX] \n",
    "                for sequence in sequences]\n",
    "    \n",
    "    def source_sequences_to_thought_vectors(self, source_sequences):\n",
    "        # From predict we get back parallel arrays for the encoder output states:\n",
    "        #\n",
    "        #    LSTM1_h, LSTM1_c, ..., LSTMn_h, LSTMn_c\n",
    "        #\n",
    "        # We want to return a list thought_vectors s.t. thought_vectors[i] \n",
    "        # returns the relevant LSTM states for source_sequences[i]. The use of\n",
    "        # zip beblow achieves this.\n",
    "        if not isinstance(source_sequences, np.ndarray):\n",
    "            source_sequences = pad_sequences(source_sequences, \n",
    "                                             dtype='int32', \n",
    "                                             padding='pre', \n",
    "                                             value=0)\n",
    "        encoder_states = self.encoder_model.predict(source_sequences)\n",
    "        thought_vectors = encoder_states_to_thought_vectors(encoder_states)\n",
    "        for thought_vector in thought_vectors:\n",
    "            assert_valid_thought_vector(thought_vector)\n",
    "        return list(thought_vectors)\n",
    "    \n",
    "    def translate_thought_vectors_to_target_sequences(self, thought_vectors, beam_length=100):\n",
    "        # The way beam_search is implemented, we only support translating \n",
    "        # one sentence at a time. Consequently, we'll need to iterate over\n",
    "        # the thought vectors and do one translation at a time.\n",
    "        target_sequences = []\n",
    "        for thought_vector in thought_vectors:\n",
    "            assert_valid_thought_vector(thought_vector)\n",
    "            sos_index = self.target_tokenizer.word_index['<s>']\n",
    "            eos_index = self.target_tokenizer.word_index['</s>']\n",
    "            oov_index = self.target_tokenizer.word_index[util.OOV_TOKEN]\n",
    "            target_sequence = beam_search(\n",
    "                thought_vector, \n",
    "                decoder_model=self.decoder_model, \n",
    "                maxlen=200,\n",
    "                sos_token_index=sos_index, \n",
    "                eos_token_index=eos_index, \n",
    "                oov_token_index=oov_index, \n",
    "                beam_length=beam_length)\n",
    "            target_sequences.append(target_sequence)\n",
    "        return target_sequences\n",
    "    \n",
    "    def translate_sequences_to_sequences(self, source_sequences, beam_length=100):\n",
    "        return self.translate_thought_vectors_to_target_sequences(\n",
    "            self.source_sequences_to_thought_vectors(source_sequences),\n",
    "            beam_length=beam_length)\n",
    "    \n",
    "    def translate_texts_to_texts(self, source_texts, beam_length=100):\n",
    "        source = self._texts_to_sequences(self.source_tokenizer, 'en', source_texts)\n",
    "        for sequence in source:\n",
    "            sequence.reverse()\n",
    "        target = self.translate_sequences_to_sequences(source, beam_length=beam_length)\n",
    "        target_texts = self.sequences_to_target_texts(target)\n",
    "        return target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_helper = InferenceHelper(encoder_model, decoder_model, \n",
    "                                   word_index_source, word_index_target,\n",
    "                                   MAX_ENGLISH_VOCAB, MAX_FRENCH_VOCAB,\n",
    "                                   'en', 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example translations using `InferenceHelper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour !',\n",
       " 'En fin de compte !',\n",
       " 'Les idées vertes ont du mal à dormir.',\n",
       " \"L'officier de police a vu l'homme avec le télescope.\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_helper.translate_texts_to_texts([\n",
    "    \"Hello!\",\n",
    "    \"Goodbye!\",\n",
    "    \"Green colorless ideas sleep furiously.\",\n",
    "    \"The police officer saw the man with the telescope.\"\n",
    "], beam_length=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we translate a subset of the dev data. We reviewed the result of this periodically to monitor how well translations improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation_summary(inference_helper, X, y, line_length=120):\n",
    "    # source sentences\n",
    "    source_sequences = [list(s) for s in X]\n",
    "    for s in source_sequences:\n",
    "        s.reverse()\n",
    "    source_texts = inference_helper.sequences_to_source_texts(source_sequences)\n",
    "\n",
    "    # translations\n",
    "    translated_sequences = [inference_helper.translate_sequences_to_sequences([sequence], beam_length=12)[0] \n",
    "                            for (sequence,label) in zip(X, y)]\n",
    "    translated_texts = inference_helper.sequences_to_target_texts(translated_sequences)\n",
    "\n",
    "    # labels\n",
    "    label_texts = inference_helper.sequences_to_target_texts(y)\n",
    "    \n",
    "    # print everything\n",
    "    for (source, label, translation) in zip(source_texts, label_texts, translated_texts):\n",
    "        place_holder = \"{0:.\" + str(line_length) + \"s}\"\n",
    "        print(place_holder.format(source))\n",
    "        print(place_holder.format(label))\n",
    "        print(place_holder.format(translation))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czechs qualified for EURO.\n",
      "Les tchèques se sont qualifiés pour l'EURO.\n",
      "Les Tchèques sont qualifiés pour l'EURO.\n",
      "\n",
      "In Montenegro they won 1: 0 and celebrate a 200 million windfall.\n",
      "Ils ont gagné au Monténégro 1 : 0 et <unk> la qualification pour 200 millions\n",
      "Au Monténégro, ils ont gagné 1 : 0 et célèbrent 200 millions d'euros.\n",
      "\n",
      "Football representation successfully managed the toughest task of the season!\n",
      "La représentation du football a réussi sa plus difficile mission de la saison !\n",
      "La représentation de football a réussi à gérer la tâche la plus difficile de la saison !\n",
      "\n",
      "<unk> watched the progress match in a detailed report.\n",
      "<unk> a suivi le match de qualification avec un reportage détaillé.\n",
      "Les participants ont examiné les progrès accomplis dans un rapport détaillé.\n",
      "\n",
      "<unk> and <unk> became the match's least lucky couple after losing two great opportunities.\n",
      "Les <unk> du match ont été <unk> et <unk> qui ont <unk> deux chances de marquer un but.\n",
      "C'est la première fois qu'il s'agit d'un des jeux les plus populaires de l'année.\n",
      "\n",
      "From the opening minutes, the game was very <unk>.\n",
      "Dès les premières minutes on a joué un football guerrier.\n",
      "D'après les heures d'ouverture, le jeu était très serré.\n",
      "\n",
      "Iranian Students Are Prepared to Die for Their Country's Nuclear Program\n",
      "Les étudiants iraniens sont prêts à mourir pour le programme nucléaire de leur pays.\n",
      "Les étudiants iraniens sont prêts à présenter un livre pour le programme nucléaire de leur pays\n",
      "\n",
      "They created a live shield.\n",
      "Ils ont créé un bouclier humain\n",
      "Ils ont créé un bouclier vivant.\n",
      "\n",
      "The gesture is to express their support of their country's nuclear program against a possible Israel attack.\n",
      "Ils veulent soutenir par ce geste le programme nucléaire de leur pays face à un éventuel assaut israélien.\n",
      "Le geste est d'exprimer leur soutien au programme nucléaire de leur pays contre une éventuelle attaque israélienne.\n",
      "\n",
      "Possible attack on Iran nuclear facilities has been discussed in Israel.\n",
      "On fait état en Israël d'une possible attaque des établissements nucléaires iraniens.\n",
      "Une éventuelle attaque contre des installations nucléaires iraniennes a été discutée en Israël.\n",
      "\n",
      "The research was performed among 900 children.\n",
      "Enquête menée sur 900 enfants\n",
      "La recherche a été menée auprès de 900 enfants.\n",
      "\n",
      "No lunch and sausage in the evening\n",
      "Sans petit déjeuner et charcuterie au dîner\n",
      "Pas de déjeuner et de saucisses dans la soirée\n",
      "\n",
      "In the Czech Republic, only 23% of schoolchildren's families have breakfast together.\n",
      "En <unk>, seulement 23 % des familles des élèves mangent à la ma même heure.\n",
      "En République tchèque, seulement 23 % des familles des écoliers ont le petit déjeuner ensemble.\n",
      "\n",
      "In a majority of families, everyone has breakfast separately.\n",
      "Dans la majeure partie des familles chacun prend son petit déjeuner seul.\n",
      "Dans la majorité des familles, tout le monde a le petit déjeuner séparément.\n",
      "\n",
      "Yet breakfast is the core of proper daily diet.\n",
      "Le petit déjeuner est pourtant la base d'une bonne alimentation quotidienne.\n",
      "Mais le petit déjeuner est le cœur d'une alimentation quotidienne saine.\n",
      "\n",
      "A survey has shown that fries are consumed more often in the families of obese children.\n",
      "Le questionnaire a montré que dans les familles des enfants obèses on consomme plus souvent des frites <unk>.\n",
      "Un sondage a montré que les frites sont consommées plus souvent dans les familles d'enfants obèses.\n",
      "\n",
      "<unk> your own menu\n",
      "<unk> toi ton alimentation\n",
      "Choisissez votre propre menu\n",
      "\n",
      "Their favourite meals include large number of flour, meat, and sweet dishes.\n",
      "Parmi leurs plats préférés il y a beaucoup de gâteaux, des viandes et des <unk>.\n",
      "Leurs repas préférés comprennent un grand nombre de farine, de viande et de plats frais.\n",
      "\n",
      "As the main dish for lunch, pasta won very tightly over poultry.\n",
      "Pour le déjeuner ce sont les pâtes qui ont pris la première place juste devant la viande blanche.\n",
      "Comme le plat principal pour le déjeuner, les pâtes sont très fortes sur la volaille.\n",
      "\n",
      "<unk> towards health\n",
      "Cap sur la santé\n",
      "Lutte contre la santé\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samp_ids = util.build_threshold_to_ids(X_dev, \n",
    "                                       y_dev, \n",
    "                                       [20], \n",
    "                                       truncate=False)[20][20:40]\n",
    "X_samp = [X_dev[i] for i in samp_ids]\n",
    "y_samp = [y_dev[i] for i in samp_ids]\n",
    "print_translation_summary(inference_helper, X_samp, y_samp, line_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Translation Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure quality of translations we use BLEU scores. Unfortunately, it turns out there are numerous ways to calculate the BLEU score, so knowing the average BLEU score a model achieves does not, on its own, tell one much. Like Sutskever, we use the `multi-bleu.perl`<sup>7</sup> for calculating BLEU scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the BLEU score of our model on both our dev (`X_dev` and `y_dev`) and test data (`X_test` and `y_test`), which correspond to the ntst1213 and ntst14 datasets respectively. (Admittedly, this relationship is unclear, and you'd have to dig into the implementation of `util.load_data(..)` to know.) To accomplish this, we generate translations of these datasets and dump them to files suitable for consumption by `multi-bleu.perl`. We do this for both a beam size of 1 and 12 for comparison with Sutskever's results. Both the `create_tokenized_translations(..)` and `generate_tokenized_translations(..)` below facilitate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_translations(inference_helper, X, beam_length, output_file):\n",
    "    import time\n",
    "    batch_size = 32\n",
    "    print(\"Writing file\", output_file)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        number_batches = int(np.ceil(len(X)/batch_size))\n",
    "        for i in range(number_batches):\n",
    "            if i % 10 == 0:\n",
    "                print(\"Processing batch\", i, \"of\", number_batches, end='\\r')\n",
    "            samples = X[(i*batch_size):(i+1)*batch_size]\n",
    "            target_sequences = inference_helper.translate_sequences_to_sequences(samples, beam_length=beam_length)\n",
    "            target_tokenized = inference_helper.sequences_to_target_tokens(target_sequences)\n",
    "            for tokenized_line in target_tokenized:\n",
    "                f.write(' '.join(tokenized_line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenized_translations(model, \n",
    "                                    inference_helper, \n",
    "                                    X, \n",
    "                                    dataset_name, \n",
    "                                    model_directory,\n",
    "                                    reference=None, \n",
    "                                    every_n_epochs=1, \n",
    "                                    start_at_epoch=0):\n",
    "    beam_lengths = [1, 12]\n",
    "    format_string = '{}.predicted.{:03d}.{}.tok'\n",
    "    regex = util.convert_format_string_to_regex(format_string)\n",
    "    \n",
    "    # find translation files we need to generate\n",
    "    params = []\n",
    "    for file, (epoch, *_) in util.list_all_weight_files_and_props(model_directory):\n",
    "        if epoch % every_n_epochs == 0 and epoch >= start_at_epoch:\n",
    "            for beam_length in beam_lengths:\n",
    "                target_file = os.path.join(\n",
    "                    model_directory, format_string.format(dataset_name, epoch, beam_length))\n",
    "                if not os.path.isfile(target_file):\n",
    "                    params.append((epoch, beam_length, file, target_file))\n",
    "    \n",
    "    # generate needed translation files\n",
    "    # save the weights so that we can restore them later\n",
    "    old_weights = list(map(np.copy, model.get_weights()))\n",
    "    try:\n",
    "        for (epoch, beam_length, weights_file, target_file) in params:\n",
    "            model.load_weights(weights_file)\n",
    "            create_tokenized_translations(inference_helper, X, beam_length, target_file)\n",
    "    finally:\n",
    "        # restore the model back to where it was\n",
    "        model.set_weights(old_weights)\n",
    "    \n",
    "    # In some cases, we may have prediction data for some epoch of the model, but not the actual\n",
    "    # model weights file (the weight file may have been deleted after generating predictions to \n",
    "    # save space). Consequently, we get all the prediction files in `model_directory` and return\n",
    "    # them, even if there is no constituent weight file.\n",
    "    translation_files = []\n",
    "    for translation_file in util.list_files_by_regex(regex, model_directory):\n",
    "        m = re.fullmatch(regex, os.path.basename(translation_file))\n",
    "        if m:\n",
    "            (dataset, epoch, beam_length) = m.groups()\n",
    "            if dataset_name == dataset:\n",
    "                translation_files.append((int(epoch), int(beam_length), translation_file))\n",
    "    \n",
    "    return sorted(translation_files, key=lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we make the appropriate calls to generate translations for ntst1213 and ntst14. For ntst1213, we generate translations not just for the final model, but for intermediate training states as well. This allows us to plot how the model performance improves throughout training. These calls will generate files in the `MODEL_WEIGHTS_DIRECTORY` containing the translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dev = generate_tokenized_translations(model,\n",
    "                                                  inference_helper,\n",
    "                                                  X_dev,\n",
    "                                                  'ntst1213',\n",
    "                                                  MODEL_WEIGHTS_DIRECTORY,\n",
    "                                                  every_n_epochs=20)\n",
    "\n",
    "predictions_test = generate_tokenized_translations(model,\n",
    "                                                   inference_helper,\n",
    "                                                   X_test,\n",
    "                                                   'ntst14',\n",
    "                                                   MODEL_WEIGHTS_DIRECTORY,\n",
    "                                                   every_n_epochs=300,\n",
    "                                                   start_at_epoch=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we calculate the the average BLEU score for the ntst1213 and ntst14 datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu_scores_from_predictions(reference_filename, predictions):\n",
    "    result = []\n",
    "    for (epoch, beam_length, prediction_file) in predictions:\n",
    "        bleu_score = util.multi_bleu(reference_filename, prediction_file)\n",
    "        result.append((epoch, beam_length, bleu_score))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_beam_bleu_dev = calc_bleu_scores_from_predictions('target/dev/ntst1213.fr', predictions_dev)\n",
    "epoch_beam_bleu_test = calc_bleu_scores_from_predictions('target/dev/ntst14.fr', predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we get the final BLEU score and the BLEU scores by sentence length on the ntst14 data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntst14_final = next(filter(lambda x: x[0] == 300 and x[1] == 12, predictions_test), None)\n",
    "lengths_bleu_test = None\n",
    "if ntst14_final:\n",
    "    length_bleu_test = util.calc_bleu_scores_by_length_thresholds('target/dev/ntst14.en',\n",
    "                                                                  'target/dev/ntst14.fr',\n",
    "                                                                  ntst14_final[-1],\n",
    "                                                                  list(range(5, 105, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU score over training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_bleu_per_epoch(epoch_beam_bleu_list, labels, title, epochs_per_iteration):\n",
    "    result = []\n",
    "    for (epoch_beam_bleu, label) in zip(epoch_beam_bleu_list, labels):\n",
    "        beam_lengths = set([beam_length for (_, beam_length, *_) in epoch_beam_bleu])\n",
    "        for beam_length in beam_lengths:\n",
    "            epoch_bleu = [(epoch, bleu) for (epoch, beam, bleu) in epoch_beam_bleu if beam == beam_length]\n",
    "            epoch_bleu = sorted(epoch_bleu, key=lambda x: x[0])\n",
    "            result.append((epoch_bleu, '{} (beam={})'.format(label, beam_length)))\n",
    "            \n",
    "    for epoch_bleu, label in result:\n",
    "        epochs, bleu_scores = zip(*epoch_bleu)\n",
    "        epochs = [epoch / epochs_per_iteration for epoch in epochs]\n",
    "        plt.plot(epochs, bleu_scores, label=label)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Bleu Score')\n",
    "    plt.title(title)\n",
    "    plt.xlim(1, epochs[-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_average_bleu_per_epoch([epoch_beam_bleu_dev], ['dev bleu score'], 'Dev Bleu Score per Iteration', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with the loss plot, the BLEU score improves gradually over time during training, with an unsurprisingly higher BLEU score with a beam size of 12 than of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU score by sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bleu_score_by_sentence_length(length_bleu, label, title):\n",
    "    lengths, bleu_scores = zip(*length_bleu)\n",
    "    range_starts = [0]\n",
    "    range_starts.extend(lengths)\n",
    "    plt.bar([i for i in range(len(lengths))], bleu_scores, width=1, label=label, align='edge')\n",
    "    plt.legend()\n",
    "    plt.xticks(range(len(range_starts)), range_starts)\n",
    "    plt.xlim(0, len(range_starts)-1)\n",
    "    plt.ylabel('Bleu Score')\n",
    "    plt.xlabel('Source Sentence Length')\n",
    "    plt.title('Bleu Score by Sentence Length')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if length_bleu_test:\n",
    "    plot_bleu_score_by_sentence_length(length_bleu_test, \n",
    "                                       'test bleu score (beam=12)', \n",
    "                                       'Bleu Score by Sentence Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sutskever does not provide data on their non-ensemble model performance by sentence length. However, they do provide this data for their ensemble model. In those results, they found that performance did not drop until exceeding a sentence length of 35 words. Here, we find performance begins to gradually drop off after 15 words. This is not an apples to apples comparison though. It would be interesting to know if using an ensemble not only improves the average performance, but also keeps performance more consistent as sentence lengths grow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final average BLEU score comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Method</th><th>test BLEU score (ntst14)</th></tr></thead><tbody><tr><td>Sutskever et al. single forward LSTM, beam size 12</td><td>26.17</td></tr><tr><td>Sutskever et al. single reversed LSTM, beam size 12</td><td>30.59</td></tr><tr><td>This implementation of single reversed LSTM, beam size 1</td><td>24.05</td></tr><tr><td><b>This implementation of single reversed LSTM, beam size 12</b></td><td><b>26.76</b></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if epoch_beam_bleu_dev and epoch_beam_bleu_test:\n",
    "    ntst1213_bleu_score = list(filter(lambda x: x[1] == 12, epoch_beam_bleu_dev))[-1][-1]\n",
    "    ntst14_b1_bleu_score = list(filter(lambda x: x[1] == 1, epoch_beam_bleu_test))[-1][-1]\n",
    "    ntst14_b12_bleu_score = list(filter(lambda x: x[1] == 12, epoch_beam_bleu_test))[-1][-1]\n",
    "    display(HTML('<table>{}</tbody></table>'.format(\n",
    "        '<thead><tr><th>Method</th><th>test BLEU score (ntst14)</th></tr></thead><tbody><tr>{}</tr>'.format(\n",
    "            '</tr><tr>'.join(\n",
    "                ['<td>Sutskever et al. single forward LSTM, beam size 12</td><td>26.17</td>',\n",
    "                 '<td>Sutskever et al. single reversed LSTM, beam size 12</td><td>30.59</td>',\n",
    "                 '<td>This implementation of single reversed LSTM, beam size 1</td><td>{}</td>',\n",
    "                 '<td><b>This implementation of single reversed LSTM, beam size 12</b></td><td><b>{}</b></td>'\n",
    "                     ]).format(ntst14_b1_bleu_score, ntst14_b12_bleu_score)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the implementation provided here under-performs Sutskever noticeably. In particular, our model produces results with reversed source sentences only slightly better than Sutskever's with non-reversed source sentences, and nearly 4 points less than Sutskever's with reversed source sentences. This might be due to the modifications made to the model, such as reducing the vocabulary size, and further compounded by not retuning the hyperparameters, such as the learning rate, accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473).\n",
    "\n",
    "2. Welin Chen, David Grangier, Michael Auli. 2015. Strategies for Training Large Vocabulary Neural Language Models. [arXiv:1512.04906](https://arxiv.org/abs/1512.04906).\n",
    "\n",
    "3. Liang Huang and Kai Zhao and Mingbo Ma. 2017. When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size). _EMNLP_.\n",
    "\n",
    "4.  H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/, 2014. \\[Online; accessed 01-May-2018\\].\n",
    "\n",
    "5. Ilya Sutskever, Oriol Vinyals, Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. [arXiv:1409.3215v3](https://arxiv.org/abs/1409.3215v3).\n",
    "\n",
    "6. George Kingsley Zipf. 1949. Human behavior and the principle of least effort.\n",
    "\n",
    "7. mosesdecoder. Github Repository, https://github.com/moses-smt/mosesdecoder. \\[Online; accessed 01-May-2018\\]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
